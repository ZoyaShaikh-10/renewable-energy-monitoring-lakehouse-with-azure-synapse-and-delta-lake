{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6da63706-ed4b-4539-b845-3053a5970628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Clean the Consumption and Production Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571437b9-0d91-4bbe-8409-169c7bb0fdff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EnergyConsumptionAndProduction\") \\\n",
    "    .config(\"fs.azure.account.key.azddevstorage.dfs.core.windows.net\", \"YOUR_STORAGE_ACCOUNT_KEY_HERE\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b149c0-cbf7-421b-a685-1cc5d70e5ed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- residential_consumption: float (nullable = true)\n",
      " |-- commercial_consumption: float (nullable = true)\n",
      " |-- industrial_consumption: float (nullable = true)\n",
      "\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+\n",
      "|               date|location|residential_consumption|commercial_consumption|industrial_consumption|\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+\n",
      "|2024-12-01 00:00:00| Chicago|                1985.61|               4316.34|               9848.08|\n",
      "|2024-12-02 00:00:00| Chicago|                 508.75|               2747.64|               7818.95|\n",
      "|2024-12-03 00:00:00| Chicago|                 925.16|               2922.28|               4226.12|\n",
      "|2024-12-04 00:00:00| Chicago|                 795.91|               2348.61|               8327.68|\n",
      "|2024-12-05 00:00:00| Chicago|                 818.12|               2592.37|               3114.77|\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a sample Parquet file for one of the locations (e.g., Boston) to inspect the data\n",
    "consumption_path_sample = \"abfss://<container-name>@<storage-acc>.dfs.core.windows.net/delta/batch/Chicago/consumption_data/location=Chicago/\"\n",
    "consumption_sample_df = spark.read.format(\"delta\").load(consumption_path_sample)\n",
    "\n",
    "# Show the schema of the Parquet data to understand the structure\n",
    "consumption_sample_df.printSchema()\n",
    "\n",
    "# Show a few sample rows to understand the data\n",
    "consumption_sample_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "702728ce-f564-413f-8593-568ffdaae441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Process Batch Data And Upload the Processed Data to Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144086a5-9f28-42b9-b28a-cf6a792bedae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined data for Boston:\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "|timestamp          |location|residential_consumption|commercial_consumption|industrial_consumption|solar_energy|wind_energy|hydro_energy|\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "|2024-12-12 00:00:00|Boston  |527.09                 |4662.28               |6496.05               |353.12      |212.93     |1881.35     |\n",
      "|2024-12-31 00:00:00|Boston  |753.72                 |4697.43               |5167.64               |350.28      |75.29      |671.94      |\n",
      "|2024-12-20 00:00:00|Boston  |872.45                 |2664.62               |7110.42               |996.17      |82.68      |1629.89     |\n",
      "|2024-12-13 00:00:00|Boston  |1826.05                |2153.57               |5563.9                |942.53      |135.85     |337.94      |\n",
      "|2024-12-06 00:00:00|Boston  |1963.94                |4596.32               |8547.22               |510.13      |416.34     |1935.24     |\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] Data for Boston ready for upload (commented out for review).\n",
      "Final combined data for New York:\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "|timestamp          |location|residential_consumption|commercial_consumption|industrial_consumption|solar_energy|wind_energy|hydro_energy|\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "|2024-12-29 00:00:00|New York|940.39                 |3906.22               |1590.54               |735.23      |473.35     |893.55      |\n",
      "|2024-12-27 00:00:00|New York|1686.38                |2274.92               |8187.05               |785.39      |364.32     |253.14      |\n",
      "|2024-12-04 00:00:00|New York|1424.62                |3901.86               |3116.11               |158.26      |180.02     |1731.82     |\n",
      "|2024-12-12 00:00:00|New York|964.65                 |4531.24               |8109.77               |727.31      |315.43     |612.35      |\n",
      "|2024-12-13 00:00:00|New York|1053.12                |4872.23               |5870.87               |994.42      |291.9      |1280.76     |\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] Data for New York ready for upload (commented out for review).\n",
      "Final combined data for San Francisco:\n",
      "+-------------------+-------------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "|timestamp          |location     |residential_consumption|commercial_consumption|industrial_consumption|solar_energy|wind_energy|hydro_energy|\n",
      "+-------------------+-------------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "|2024-12-27 00:00:00|San Francisco|1120.68                |4976.21               |9930.11               |997.9       |412.97     |1441.23     |\n",
      "|2024-12-06 00:00:00|San Francisco|1123.34                |4867.61               |2153.33               |723.76      |187.32     |975.67      |\n",
      "|2024-12-18 00:00:00|San Francisco|1026.87                |2565.1                |8493.31               |889.18      |256.83     |1637.2      |\n",
      "|2024-12-08 00:00:00|San Francisco|1254.54                |4761.69               |9798.09               |147.09      |473.45     |1813.09     |\n",
      "|2024-12-14 00:00:00|San Francisco|1557.56                |4422.02               |2099.9                |890.59      |358.01     |1070.91     |\n",
      "+-------------------+-------------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] Data for San Francisco ready for upload (commented out for review).\n",
      "Final combined data for Chicago:\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "|timestamp          |location|residential_consumption|commercial_consumption|industrial_consumption|solar_energy|wind_energy|hydro_energy|\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "|2024-12-03 00:00:00|Chicago |925.16                 |2922.28               |4226.12               |757.71      |398.09     |1888.12     |\n",
      "|2024-12-01 00:00:00|Chicago |1985.61                |4316.34               |9848.08               |631.86      |107.99     |1114.51     |\n",
      "|2024-12-09 00:00:00|Chicago |689.67                 |4943.28               |4956.73               |756.21      |80.3       |1940.91     |\n",
      "|2024-12-12 00:00:00|Chicago |1852.28                |2014.66               |6748.72               |227.77      |205.19     |973.01      |\n",
      "|2024-12-15 00:00:00|Chicago |524.69                 |3863.53               |8330.88               |735.92      |260.37     |1394.84     |\n",
      "+-------------------+--------+-----------------------+----------------------+----------------------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] Data for Chicago ready for upload (commented out for review).\n",
      "[INFO] Data processing for all locations completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Paths for batch data (consumption and production) and processed data\n",
    "batch_consumption_path = \"abfss://<container-name>@<storage-acc>.dfs.core.windows.net/delta/batch/{location}/consumption_data/\"\n",
    "batch_production_path = \"abfss://<container-name>@<storage-acc>.dfs.core.windows.net/delta/batch/{location}/production_data/\"\n",
    "processed_batch_data_path = \"abfss://<container-name>@<storage-acc>.dfs.core.windows.net/delta/processed/batch/{location}/\"\n",
    "\n",
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"EnergyDataProcessing\").getOrCreate()\n",
    "\n",
    "# Function to process and combine batch consumption and production data\n",
    "def process_and_combine_batch_data(location):\n",
    "    try:\n",
    "        # Load the batch consumption and production data from Delta (without specifying a schema)\n",
    "        consumption_df = spark.read.format(\"delta\").load(batch_consumption_path.format(location=location))\n",
    "        production_df = spark.read.format(\"delta\").load(batch_production_path.format(location=location))\n",
    "\n",
    "        # Perform transformations on consumption data based of the type\n",
    "\n",
    "       \n",
    "        consumption_clean_df = consumption_df.withColumnRenamed(\"date\", \"timestamp\") \\\n",
    "            .withColumn(\"residential_consumption\", F.col(\"residential_consumption\").cast(\"float\")) \\\n",
    "            .withColumn(\"commercial_consumption\", F.col(\"commercial_consumption\").cast(\"float\")) \\\n",
    "            .withColumn(\"industrial_consumption\", F.col(\"industrial_consumption\").cast(\"float\")) \\\n",
    "            .dropna(subset=[\"timestamp\", \"residential_consumption\", \"commercial_consumption\", \"industrial_consumption\"])\n",
    "        # else:\n",
    "        #     cleaned_consumption_df = consumption_real_time_df \\\n",
    "        #         .withColumnRenamed(\"time\", \"timestamp\") \\\n",
    "        #         .withColumn(\"energy_consumption_kWh\", F.col(\"energy_consumption_kWh\").cast(\"float\")) \\\n",
    "        #         .dropna(subset=[\"timestamp\", \"energy_consumption_kWh\"])  # Drop rows where timestamp or energy_consumption_kWh is null\n",
    "\n",
    "        # Perform transformations on production data\n",
    "        production_clean_df = production_df.withColumnRenamed(\"time\", \"timestamp\") \\\n",
    "            .withColumn(\"solar_energy\", F.col(\"solar_energy\").cast(\"float\")) \\\n",
    "            .withColumn(\"wind_energy\", F.col(\"wind_energy\").cast(\"float\")) \\\n",
    "            .withColumn(\"hydro_energy\", F.col(\"hydro_energy\").cast(\"float\")) \\\n",
    "            .dropna(subset=[\"timestamp\", \"solar_energy\", \"wind_energy\", \"hydro_energy\"])\n",
    "\n",
    "        # Combine both cleaned DataFrames (consumption and production)\n",
    "        combined_df = consumption_clean_df.join(production_clean_df, on=[\"timestamp\", \"location\"], how=\"outer\")\n",
    "\n",
    "        # Show the final combined DataFrame (before uploading to Delta)\n",
    "        print(f\"Final combined data for {location}:\")\n",
    "        combined_df.show(5, truncate=False)\n",
    "\n",
    "        # Uncomment below to save the combined data to Delta\n",
    "        combined_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"location\").save(processed_batch_data_path.format(location=location))\n",
    "\n",
    "        print(f\"[INFO] Data for {location} ready for upload (commented out for review).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error processing data for {location}: {e}\")\n",
    "\n",
    "# Loop through all locations and process data\n",
    "locations = [\"Boston\", \"New York\", \"San Francisco\", \"Chicago\"]\n",
    "for location in locations:\n",
    "    process_and_combine_batch_data(location)\n",
    "\n",
    "print(\"[INFO] Data processing for all locations completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79af14c9-1f7f-479a-86dd-303608a68d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- solar_energy: float (nullable = true)\n",
      " |-- wind_energy: float (nullable = true)\n",
      " |-- hydro_energy: float (nullable = true)\n",
      "\n",
      "+-------------------+--------+------------+-----------+------------+\n",
      "|               time|location|solar_energy|wind_energy|hydro_energy|\n",
      "+-------------------+--------+------------+-----------+------------+\n",
      "|2025-01-10 15:40:00|  Boston|       34.91|     276.25|         0.0|\n",
      "|2025-01-10 15:41:00|  Boston|       35.07|     275.42|         0.0|\n",
      "|2025-01-10 15:42:00|  Boston|       35.23|     274.58|         0.0|\n",
      "|2025-01-10 15:43:00|  Boston|        35.4|     273.75|         0.0|\n",
      "|2025-01-10 15:44:00|  Boston|       35.56|     272.92|         0.0|\n",
      "+-------------------+--------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a sample Parquet file for one of the locations (e.g., Boston) to inspect the data for processes\n",
    "consumption_path_sample = \"abfss://<container-name>@<storage-acc>.dfs.core.windows.net/delta/real-time/Boston/production_simulation_data/location=Boston\"\n",
    "consumption_sample_df = spark.read.format(\"delta\").load(consumption_path_sample)\n",
    "\n",
    "# Show the schema of the Parquet data to understand the structure\n",
    "consumption_sample_df.printSchema()\n",
    "\n",
    "# Show a few sample rows to understand the data\n",
    "consumption_sample_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5461e956-8065-45c5-a6f8-d9a2442316a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processing Real-Time Data and Uploading the Processed Data to the Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d16743-3f94-4967-868f-366374bcfa34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined data for Boston:\n",
      "+--------------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "|timestamp                 |location|latitude|longitude|country|energy_consumption_kWh|solar_energy|wind_energy|hydro_energy|solar_power|\n",
      "+--------------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "|2025-01-12 07:12:32.752234|Boston  |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 04:12:32.752234|Boston  |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 03:12:32.752234|Boston  |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 12:12:32.752234|Boston  |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 02:12:32.752234|Boston  |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "+--------------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] Data for Boston ready for upload (commented out for review).\n",
      "Final combined data for New York:\n",
      "+--------------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "|timestamp                 |location|latitude|longitude|country|energy_consumption_kWh|solar_energy|wind_energy|hydro_energy|solar_power|\n",
      "+--------------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "|2025-01-12 07:12:32.752234|Boston  |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 04:12:32.752234|Boston  |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 03:12:32.752234|Boston  |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 12:12:32.752234|Boston  |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 02:12:32.752234|Boston  |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "+--------------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] Data for New York ready for upload (commented out for review).\n",
      "Final combined data for San Francisco:\n",
      "+--------------------------+-------------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "|timestamp                 |location     |latitude|longitude|country|energy_consumption_kWh|solar_energy|wind_energy|hydro_energy|solar_power|\n",
      "+--------------------------+-------------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "|2025-01-12 07:12:32.752234|Boston       |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-11 17:12:32.82742 |San Francisco|37.7749 |-122.4194|USA    |100.0                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 01:12:32.82742 |San Francisco|37.7749 |-122.4194|USA    |100.0                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 04:12:32.752234|Boston       |42.3601 |-71.0589 |USA    |155.3                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 10:12:32.82742 |San Francisco|37.7749 |-122.4194|USA    |100.0                 |NULL        |NULL       |NULL        |NULL       |\n",
      "+--------------------------+-------------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] Data for San Francisco ready for upload (commented out for review).\n",
      "Final combined data for Chicago:\n",
      "+--------------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "|timestamp                 |location|latitude|longitude|country|energy_consumption_kWh|solar_energy|wind_energy|hydro_energy|solar_power|\n",
      "+--------------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "|2025-01-12 04:12:32.839599|Chicago |41.8781 |-87.6298 |USA    |130.0                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-11 19:12:32.839599|Chicago |41.8781 |-87.6298 |USA    |150.0                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-11 18:12:32.839599|Chicago |41.8781 |-87.6298 |USA    |150.0                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-11 17:12:32.839599|Chicago |41.8781 |-87.6298 |USA    |130.0                 |NULL        |NULL       |NULL        |NULL       |\n",
      "|2025-01-12 05:12:32.839599|Chicago |41.8781 |-87.6298 |USA    |130.0                 |NULL        |NULL       |NULL        |NULL       |\n",
      "+--------------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] Data for Chicago ready for upload (commented out for review).\n",
      "[INFO] Data processing for all locations completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Paths for batch data (consumption and production) and processed data\n",
    "real_time_consumption_path = \"abfss://<container-name>@<storage-acc>.dfs.core.windows.net/delta/real-time/{location}/energy_consumption_data/\"\n",
    "real_time_production_path = \"abfss://<container-name>@<storage-acc>.dfs.core.windows.net/delta/real-time/{location}/production_simulation_data/\"\n",
    "processed_real_time_data_path = \"abfss://<container-name>@<storage-acc>.dfs.core.windows.net/delta/processed/real-time/{location}/\"\n",
    "\n",
    "# Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"EnergyDataProcessing\").getOrCreate()\n",
    "\n",
    "# Function to process and combine batch consumption and production data\n",
    "def process_and_combine_real_time_data(location):\n",
    "    try:\n",
    "        # Load the batch consumption and production data from Delta (without specifying a schema)\n",
    "        consumption_df = spark.read.format(\"delta\").load(real_time_consumption_path.format(location=location))\n",
    "        production_df = spark.read.format(\"delta\").load(real_time_production_path.format(location=location))\n",
    "\n",
    "        # Perform transformations on consumption data based of the type\n",
    "\n",
    "       \n",
    "        # consumption_clean_df = consumption_df.withColumnRenamed(\"date\", \"timestamp\") \\\n",
    "        #     .withColumn(\"residential_consumption\", F.col(\"residential_consumption\").cast(\"float\")) \\\n",
    "        #     .withColumn(\"commercial_consumption\", F.col(\"commercial_consumption\").cast(\"float\")) \\\n",
    "        #     .withColumn(\"industrial_consumption\", F.col(\"industrial_consumption\").cast(\"float\")) \\\n",
    "        #     .dropna(subset=[\"timestamp\", \"residential_consumption\", \"commercial_consumption\", \"industrial_consumption\"])\n",
    "        # # else:\n",
    "        consumption_clean_df = consumption_df \\\n",
    "            .withColumnRenamed(\"time\", \"timestamp\") \\\n",
    "            .withColumn(\"energy_consumption_kWh\", F.col(\"energy_consumption_kWh\").cast(\"float\")) \\\n",
    "            .dropna(subset=[\"timestamp\", \"energy_consumption_kWh\"])  # Drop rows where timestamp or energy_consumption_kWh is null\n",
    "\n",
    "        # Perform transformations on production data\n",
    "        production_clean_df = production_df.withColumnRenamed(\"time\", \"timestamp\") \\\n",
    "            .withColumn(\"solar_power\", F.col(\"solar_energy\").cast(\"float\")) \\\n",
    "            .withColumn(\"wind_energy\", F.col(\"wind_energy\").cast(\"float\")) \\\n",
    "            .withColumn(\"hydro_energy\", F.col(\"hydro_energy\").cast(\"float\")) \\\n",
    "            .dropna(subset=[\"timestamp\", \"solar_energy\", \"wind_energy\", \"hydro_energy\"])\n",
    "\n",
    "        # Combine both cleaned DataFrames (consumption and production)\n",
    "        combined_df = consumption_clean_df.join(production_clean_df, on=[\"timestamp\", \"location\"], how=\"outer\")\n",
    "\n",
    "        # Show the final combined DataFrame (before uploading to Delta)\n",
    "        print(f\"Final combined data for {location}:\")\n",
    "        combined_df.show(5, truncate=False)\n",
    "\n",
    "        # Uncomment below to save the combined data to Delta\n",
    "        combined_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(\"location\").save(processed_real_time_data_path.format(location=location))\n",
    "\n",
    "        print(f\"[INFO] Data for {location} ready for upload (commented out for review).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error processing data for {location}: {e}\")\n",
    "\n",
    "# Loop through all locations and process data\n",
    "locations = [\"Boston\", \"New York\", \"San Francisco\", \"Chicago\"]\n",
    "for location in locations:\n",
    "    process_and_combine_real_time_data(location)\n",
    "\n",
    "print(\"[INFO] Data processing for all locations completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "338500f1-7f22-434a-a451-6b53f55cfe63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- energy_consumption_kWh: float (nullable = true)\n",
      " |-- solar_energy: float (nullable = true)\n",
      " |-- wind_energy: float (nullable = true)\n",
      " |-- hydro_energy: float (nullable = true)\n",
      " |-- solar_power: float (nullable = true)\n",
      "\n",
      "+--------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "|           timestamp|location|latitude|longitude|country|energy_consumption_kWh|solar_energy|wind_energy|hydro_energy|solar_power|\n",
      "+--------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "|2025-01-12 07:12:...|  Boston| 42.3601| -71.0589|    USA|                 155.3|        NULL|       NULL|        NULL|       NULL|\n",
      "|2025-01-12 04:12:...|  Boston| 42.3601| -71.0589|    USA|                 155.3|        NULL|       NULL|        NULL|       NULL|\n",
      "|2025-01-12 03:12:...|  Boston| 42.3601| -71.0589|    USA|                 155.3|        NULL|       NULL|        NULL|       NULL|\n",
      "|2025-01-12 12:12:...|  Boston| 42.3601| -71.0589|    USA|                 155.3|        NULL|       NULL|        NULL|       NULL|\n",
      "|2025-01-12 02:12:...|  Boston| 42.3601| -71.0589|    USA|                 155.3|        NULL|       NULL|        NULL|       NULL|\n",
      "+--------------------+--------+--------+---------+-------+----------------------+------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a sample Parquet file for one of the locations (e.g., Boston) to inspect the data for processes\n",
    "consumption_path_sample = \"abfss://<container-name>@<storage-acc>.dfs.core.windows.net/delta/processed/real-time/Boston/location=Boston\"\n",
    "consumption_sample_df = spark.read.format(\"delta\").load(consumption_path_sample)\n",
    "\n",
    "# Show the schema of the Parquet data to understand the structure\n",
    "consumption_sample_df.printSchema()\n",
    "\n",
    "# Show a few sample rows to understand the data\n",
    "consumption_sample_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8836070c-5c34-4607-acf9-db456028af15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "energy_consumption_and_production_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
